# 184 are kept with m try = p
# sqrt p keeps 490
keep_vars = importance(rf, type = 2)
feat_all = rownames(keep_vars)
feat_keep = feat_all[keep_vars > 1]
group$Train$X = group$Train$X[, colnames(group$Train$X) %in% feat_keep]
group$Test$X = group$Test$X[, colnames(group$Test$X) %in% feat_keep]
return(group)
}
start_time = proc.time()
Group = machine_learning(Group, importance_selection, the_stack_carrot)
the_stack_carrot <- function(group = Group, models = 5){
matrix = matrix(0, nrow = nrow(group$Test$X), ncol = models)
df_predictions = as.data.frame(matrix)
final_predictions = vector("double", nrow(group$Test$X))
fitControl = trainControl('none', verboseIter = FALSE, allowParallel = TRUE)
datControl = NULL
gridControl = expand.grid(mtry = sqrt(ncol(group$Train$X)))
method_fit = train(group$Train$X, factor(group$Train$Y),
method = 'rf',
preProcess = datControl,
trControl = fitControl,
tuneGrid = gridControl,
ntree = 500)
df_predictions[, 1] = predict(method_fit, group$Test$X)
cat(1, "\n")
method_fit = train(group$Train$X %>% scale(), factor(group$Train$Y), method = "lda",
preProcess = datControl, trControl = fitControl)
df_predictions[, 2] = predict(method_fit, newdata = group$Test$X %>% scale())
cat(2, "\n")
fitControl = trainControl(method = "cv", number = 5, search = "grid", allowParallel = TRUE)
gridControl = expand.grid(cost = 2^(c(2, 4, 6)))
datControl = NULL
svm_fit = train(group$Train$X %>% scale(), factor(group$Train$Y), method = "svmLinear2",
trControl = fitControl, tuneGrid = gridControl,
preProcess = datControl)
df_predictions[, 3] = predict(svm_fit, newdata = group$Test$X %>% scale())
cat(3, "\n")
gridControl = expand.grid(mtry = ncol(group$Train$X))
method_fit = train(group$Train$X, factor(group$Train$Y),
method = 'rf',
preProcess = datControl,
trControl = fitControl,
tuneGrid = gridControl,
ntree = 500)
df_predictions[, 4] = predict(svm_fit, newdata = group$Test$X %>% scale())
cat(4, "\n")
for(row in 1:nrow(df_predictions)){
temp = df_predictions[row, ] %>% as.matrix() %>% table()
pred = which.max(temp) %>% names() %>% as.double()
final_predictions[row] = pred
}
group$Test$CrystalBall = final_predictions
group$Test$STACK = df_predictions
group$Test$Method = method_fit
return(group)
}
importance_selection <- function(group = Group){
# all feature selection functions will return a new list with features selected
library(randomForest)
rf = randomForest(group$Train$X, factor(group$Train$Y),
ntree = 500,
mtry = sqrt(ncol(group$Train$X))
)
# 184 are kept with m try = p
# sqrt p keeps 490
keep_vars = importance(rf, type = 2)
feat_all = rownames(keep_vars)
feat_keep = feat_all[keep_vars > 1]
group$Train$X = group$Train$X[, colnames(group$Train$X) %in% feat_keep]
group$Test$X = group$Test$X[, colnames(group$Test$X) %in% feat_keep]
return(group)
}
library(tidyverse)
library(caret)
library(R.utils)
library(parallel)
library(doParallel)
# cores
cluster = makeCluster(detectCores() - 1)
registerDoParallel(cluster)
# source
sourceDirectory(paste0(getwd(), '/Functions/Feature Selection/'))
sourceDirectory(paste0(getwd(), '/Functions/Models/'))
sourceDirectory(paste0(getwd(), '/Functions/Pipeline/'))
sourceDirectory(paste0(getwd(), '/Functions/Split into Groups/'))
sourceDirectory(paste0(getwd(), '/Functions/Others/'))
# read
dat_train <- read_csv(paste0(getwd(), '/Data/cooked_training_data.csv'))
dat_test <- read_csv(paste0(getwd(), '/Data/cooked_test_data.csv'))
Train = get_group_data(dat_train, group_labs = 1:6)
Test = list("X" = dat_test)
Group = list("Train" = Train, "Test" = Test)
compare_2_benchmark <- function(my_preds, name = "The Tree of Michael (No Feature Selection and Scaled, Benchmark)"){
path = paste0(getwd(), "/Predictions/")
preds = read_csv(paste0(path, name, ".csv"))
cat(sum(my_preds != preds$Prediction), "\n")
}
start_time = proc.time()
Group = machine_learning(Group, importance_selection, the_stack_carrot)
2^(-10:-5)
library(tidyverse)
library(caret)
library(R.utils)
library(parallel)
library(doParallel)
# cores
cluster = makeCluster(detectCores() - 1)
registerDoParallel(cluster)
# source
sourceDirectory(paste0(getwd(), '/Functions/Feature Selection/'))
sourceDirectory(paste0(getwd(), '/Functions/Models/'))
sourceDirectory(paste0(getwd(), '/Functions/Pipeline/'))
sourceDirectory(paste0(getwd(), '/Functions/Split into Groups/'))
sourceDirectory(paste0(getwd(), '/Functions/Others/'))
# read
dat_train <- read_csv(paste0(getwd(), '/Data/cooked_training_data.csv'))
dat_test <- read_csv(paste0(getwd(), '/Data/cooked_test_data.csv'))
Train = get_group_data(dat_train, group_labs = 1:6)
Test = list("X" = dat_test)
Group = list("Train" = Train, "Test" = Test)
start_time = proc.time()
install.packages("kernlab")
library(tidyverse)
library(caret)
library(R.utils)
library(parallel)
library(doParallel)
# cores
cluster = makeCluster(detectCores() - 1)
registerDoParallel(cluster)
# source
sourceDirectory(paste0(getwd(), '/Functions/Feature Selection/'))
sourceDirectory(paste0(getwd(), '/Functions/Models/'))
sourceDirectory(paste0(getwd(), '/Functions/Pipeline/'))
sourceDirectory(paste0(getwd(), '/Functions/Split into Groups/'))
sourceDirectory(paste0(getwd(), '/Functions/Others/'))
# read
dat_train <- read_csv(paste0(getwd(), '/Data/cooked_training_data.csv'))
dat_test <- read_csv(paste0(getwd(), '/Data/cooked_test_data.csv'))
Train = get_group_data(dat_train, group_labs = 1:6)
Test = list("X" = dat_test)
Group = list("Train" = Train, "Test" = Test)
start_time = proc.time()
the_stack_carrot <- function(group = Group, models = 6){
matrix = matrix(0, nrow = nrow(group$Test$X), ncol = models)
df_predictions = as.data.frame(matrix)
final_predictions = vector("double", nrow(group$Test$X))
fitControl = trainControl('none', verboseIter = FALSE, allowParallel = TRUE)
datControl = NULL
gridControl = expand.grid(mtry = sqrt(ncol(group$Train$X)))
method_fit = train(group$Train$X, factor(group$Train$Y),
method = 'rf',
preProcess = datControl,
trControl = fitControl,
tuneGrid = gridControl,
ntree = 500)
df_predictions[, 1] = predict(method_fit, group$Test$X)
cat(1, "\n")
method_fit = train(group$Train$X %>% scale(), factor(group$Train$Y), method = "lda",
preProcess = datControl, trControl = fitControl)
df_predictions[, 2] = predict(method_fit, newdata = group$Test$X %>% scale())
cat(2, "\n")
fitControl = trainControl(method = "cv", number = 2, search = "grid", allowParallel = TRUE)
gridControl = expand.grid(cost = 2^(c(2, 4, 6)))
datControl = NULL
svm_fit = train(group$Train$X %>% scale(), factor(group$Train$Y), method = "svmLinear2",
trControl = fitControl, tuneGrid = gridControl,
preProcess = datControl)
df_predictions[, 3] = predict(svm_fit, newdata = group$Test$X %>% scale())
cat(3, "\n")
gridControl = expand.grid(mtry = ncol(group$Train$X))
method_fit = train(group$Train$X, factor(group$Train$Y),
method = 'rf',
preProcess = datControl,
trControl = fitControl,
tuneGrid = gridControl,
ntree = 500)
df_predictions[, 4] = predict(method_fit, newdata = group$Test$X %>% scale())
cat(4, "\n")
fitControl = trainControl(method = "cv", number = 2, search = "grid", allowParallel = TRUE)
gridControl = expand.grid(C = 2^(c(2, 4, 6)), sigma = 0.0009765625)
datControl = NULL
svm_fit = train(group$Train$X %>% scale(), factor(group$Train$Y), method = "svmRadial",
trControl = fitControl, tuneGrid = gridControl,
preProcess = datControl)
df_predictions[, 5] = predict(svm_fit, newdata = group$Test$X %>% scale())
cat(6, "\n")
fitControl = trainControl(method = "none", allowParallel = TRUE)
gridControl = expand.grid(maxInteractionOrder = 1)
datControl = NULL
method_fit = train(group$Train$X %>% scale(), factor(group$Train$Y), method = "randomGLM",
trControl = fitControl, tuneGrid = gridControl,
preProcess = datControl)
df_predictions[, 6] = predict(method_fit, newdata = group$Test$X %>% scale())
cat(5, "\n")
for(row in 1:nrow(df_predictions)){
temp = df_predictions[row, ] %>% as.matrix() %>% table()
pred = which.max(temp) %>% names() %>% as.double()
final_predictions[row] = pred
}
group$Test$CrystalBall = final_predictions
group$Test$STACK = df_predictions
group$Test$Method = method_fit
return(group)
}
source('~/Documents/Academics/Rice University/Semesters/Fall 2017/STAT 413/Project/Crystal Ballers/Functions/Others/compare_2_benchmarks.R')
source('~/Documents/Academics/Rice University/Semesters/Fall 2017/STAT 413/Project/Crystal Ballers/Functions/Feature Selection/importance_selection.R')
library(tidyverse)
library(caret)
library(R.utils)
library(parallel)
library(doParallel)
# cores
cluster = makeCluster(detectCores() - 1)
registerDoParallel(cluster)
# source
sourceDirectory(paste0(getwd(), '/Functions/Feature Selection/'))
sourceDirectory(paste0(getwd(), '/Functions/Models/'))
sourceDirectory(paste0(getwd(), '/Functions/Pipeline/'))
sourceDirectory(paste0(getwd(), '/Functions/Split into Groups/'))
sourceDirectory(paste0(getwd(), '/Functions/Others/'))
# read
dat_train <- read_csv(paste0(getwd(), '/Data/cooked_training_data.csv'))
dat_test <- read_csv(paste0(getwd(), '/Data/cooked_test_data.csv'))
Train = get_group_data(dat_train, group_labs = 1:6)
Test = list("X" = dat_test)
Group = list("Train" = Train, "Test" = Test)
start_time = proc.time()
the_stack_carrot <- function(group = Group, models = 6){
matrix = matrix(0, nrow = nrow(group$Test$X), ncol = models)
df_predictions = as.data.frame(matrix)
final_predictions = vector("double", nrow(group$Test$X))
fitControl = trainControl('none', verboseIter = FALSE, allowParallel = TRUE)
datControl = NULL
gridControl = expand.grid(mtry = sqrt(ncol(group$Train$X)))
method_fit = train(group$Train$X, factor(group$Train$Y),
method = 'rf',
preProcess = datControl,
trControl = fitControl,
tuneGrid = gridControl,
ntree = 500)
df_predictions[, 1] = predict(method_fit, group$Test$X)
cat(1, "\n")
method_fit = train(group$Train$X %>% scale(), factor(group$Train$Y), method = "lda",
preProcess = datControl, trControl = fitControl)
df_predictions[, 2] = predict(method_fit, newdata = group$Test$X %>% scale())
cat(2, "\n")
fitControl = trainControl(method = "cv", number = 2, search = "grid", allowParallel = TRUE)
gridControl = expand.grid(cost = 2^(c(2, 4, 6)))
datControl = NULL
svm_fit = train(group$Train$X %>% scale(), factor(group$Train$Y), method = "svmLinear2",
trControl = fitControl, tuneGrid = gridControl,
preProcess = datControl)
df_predictions[, 3] = predict(svm_fit, newdata = group$Test$X %>% scale())
cat(3, "\n")
gridControl = expand.grid(mtry = ncol(group$Train$X))
method_fit = train(group$Train$X, factor(group$Train$Y),
method = 'rf',
preProcess = datControl,
trControl = fitControl,
tuneGrid = gridControl,
ntree = 500)
df_predictions[, 4] = predict(method_fit, newdata = group$Test$X %>% scale())
cat(4, "\n")
fitControl = trainControl(method = "cv", number = 2, search = "grid", allowParallel = TRUE)
gridControl = expand.grid(C = 2^(c(2, 4, 6)), sigma = 0.0009765625)
datControl = NULL
svm_fit = train(group$Train$X %>% scale(), factor(group$Train$Y), method = "svmRadial",
trControl = fitControl, tuneGrid = gridControl,
preProcess = datControl)
df_predictions[, 5] = predict(svm_fit, newdata = group$Test$X %>% scale())
cat(6, "\n")
fitControl = trainControl(method = "none", allowParallel = TRUE)
gridControl = expand.grid(maxInteractionOrder = 1)
datControl = NULL
method_fit = train(group$Train$X %>% scale(), factor(group$Train$Y), method = "randomGLM",
trControl = fitControl, tuneGrid = gridControl,
preProcess = datControl)
df_predictions[, 6] = predict(method_fit, newdata = group$Test$X %>% scale())
cat(5, "\n")
for(row in 1:nrow(df_predictions)){
temp = df_predictions[row, ] %>% as.matrix() %>% table()
pred = which.max(temp) %>% names() %>% as.double()
final_predictions[row] = pred
}
group$Test$CrystalBall = final_predictions
group$Test$STACK = df_predictions
group$Test$Method = method_fit
return(group)
}
source('~/Documents/Academics/Rice University/Semesters/Fall 2017/STAT 413/Project/Crystal Ballers/Functions/Feature Selection/importance_selection.R')
source('~/Documents/Academics/Rice University/Semesters/Fall 2017/STAT 413/Project/Crystal Ballers/Functions/Others/compare_2_benchmarks.R')
start_time = proc.time()
Group = machine_learning(Group, importance_selection, the_stack_carrot)
library(tidyverse)
library(caret)
library(R.utils)
library(parallel)
library(doParallel)
# cores
cluster = makeCluster(detectCores() - 1)
registerDoParallel(cluster)
# source
sourceDirectory(paste0(getwd(), '/Functions/Feature Selection/'))
sourceDirectory(paste0(getwd(), '/Functions/Models/'))
sourceDirectory(paste0(getwd(), '/Functions/Pipeline/'))
sourceDirectory(paste0(getwd(), '/Functions/Split into Groups/'))
sourceDirectory(paste0(getwd(), '/Functions/Others/'))
# read
dat_train <- read_csv(paste0(getwd(), '/Data/cooked_training_data.csv'))
dat_test <- read_csv(paste0(getwd(), '/Data/cooked_test_data.csv'))
Train = get_group_data(dat_train, group_labs = 1:6)
Test = list("X" = dat_test)
Group = list("Train" = Train, "Test" = Test)
start_time = proc.time()
the_stack_carrot <- function(group = Group, models = 5){
matrix = matrix(0, nrow = nrow(group$Test$X), ncol = models)
df_predictions = as.data.frame(matrix)
final_predictions = vector("double", nrow(group$Test$X))
fitControl = trainControl('none', verboseIter = FALSE, allowParallel = TRUE)
datControl = NULL
gridControl = expand.grid(mtry = sqrt(ncol(group$Train$X)))
method_fit = train(group$Train$X, factor(group$Train$Y),
method = 'rf',
preProcess = datControl,
trControl = fitControl,
tuneGrid = gridControl,
ntree = 500)
df_predictions[, 1] = predict(method_fit, group$Test$X)
cat(1, "\n")
method_fit = train(group$Train$X %>% scale(), factor(group$Train$Y), method = "lda",
preProcess = datControl, trControl = fitControl)
df_predictions[, 2] = predict(method_fit, newdata = group$Test$X %>% scale())
cat(2, "\n")
fitControl = trainControl(method = "cv", number = 2, search = "grid", allowParallel = TRUE)
gridControl = expand.grid(cost = 2^(c(2, 4, 6)))
datControl = NULL
svm_fit = train(group$Train$X %>% scale(), factor(group$Train$Y), method = "svmLinear2",
trControl = fitControl, tuneGrid = gridControl,
preProcess = datControl)
df_predictions[, 3] = predict(svm_fit, newdata = group$Test$X %>% scale())
cat(3, "\n")
fitControl = trainControl(method = "none", allowParallel = TRUE)
gridControl = expand.grid(mtry = ncol(group$Train$X))
method_fit = train(group$Train$X, factor(group$Train$Y),
method = 'rf',
preProcess = datControl,
trControl = fitControl,
tuneGrid = gridControl,
ntree = 500)
df_predictions[, 4] = predict(method_fit, newdata = group$Test$X %>% scale())
cat(4, "\n")
fitControl = trainControl(method = "cv", number = 2, search = "grid", allowParallel = TRUE)
gridControl = expand.grid(C = 2^(c(2, 4, 6)), sigma = 0.0009765625)
datControl = NULL
svm_fit = train(group$Train$X %>% scale(), factor(group$Train$Y), method = "svmRadial",
trControl = fitControl, tuneGrid = gridControl,
preProcess = datControl)
df_predictions[, 5] = predict(svm_fit, newdata = group$Test$X %>% scale())
cat(5, "\n")
for(row in 1:nrow(df_predictions)){
temp = df_predictions[row, ] %>% as.matrix() %>% table()
pred = which.max(temp) %>% names() %>% as.double()
final_predictions[row] = pred
}
group$Test$CrystalBall = final_predictions
group$Test$STACK = df_predictions
group$Test$Method = method_fit
return(group)
}
source('~/Documents/Academics/Rice University/Semesters/Fall 2017/STAT 413/Project/Crystal Ballers/Functions/Others/compare_2_benchmarks.R')
source('~/Documents/Academics/Rice University/Semesters/Fall 2017/STAT 413/Project/Crystal Ballers/Functions/Feature Selection/importance_selection.R')
start_time = proc.time()
Group = machine_learning(Group, importance_selection, the_stack_carrot)
end_time = proc.time()
(end_time - start_time) / 60 / 60 # hours
source('~/Documents/Academics/Rice University/Semesters/Fall 2017/STAT 413/Project/Crystal Ballers/Functions/Others/compare_2_benchmarks.R')
compare_2_benchmark(Group$Test$CrystalBall)
Group$Test$CrystalBall
create_submission_file("THE STACK (FINAL!!!!!)", Group$Test$CrystalBall)
stopCluster(cluster)
create_submission_file("THE STACK (FINAL!!!!!)", Group$Test$CrystalBall)
create_submission_file("THE STACK (FINAL!!!!!)", Group$Test$CrystalBall)
compare_2_benchmark(Group$Test$CrystalBall)
svm_star <- function(group = Group){
# group$Train$Y = group$Train$Y[1:200]
# group$Train$X = group$Train$X[1:200, ] %>% scale()
# group$Test$X = group$Test$X[1:200, ] %>% scale()
group$Train$X = group$Train$X %>% scale()
group$Test$X = group$Test$X %>% scale()
cost_levels2 = 2^(3:10)
gamma_levels2 = 2^(-12:-6)
divide_and_conquer2 = 10
k_fold_cv2 = 10
gamma_star2 = .015625
library(e1071)
divide_and_conquer <- function(cost_levels3 = cost_levels2,
gamma_levels3 = gamma_levels2,
divide_and_conquer3 = divide_and_conquer2,
k_fold_cv3 = k_fold_cv2,
gamma_star3 = gamma_star2){
for(index_me in 1:divide_and_conquer3){
svm_tune = tune(svm, train.x=group$Train$X, train.y=factor(group$Train$Y), ranges=list(cost=cost_levels3, gamma=gamma_star3), tunecontrol=tune.control(cross=k_fold_cv3), kernel="radial", scale=FALSE)
# Divide and Conquer
exp_star = log2(svm_tune$best.parameters$cost)
range = (exp_star - 3):(exp_star + 3)
cost_levels3 = 2^range
cost_star3 = svm_tune$best.parameters$cost
if(index_me == 1){
cost_star_previous = cost_star3
}
else{
if(cost_star_previous == cost_star3)
break
else
cost_star_previous = cost_star3
}
cat(index_me, "\n")
}
for(index_me in 1:divide_and_conquer3){
svm_tune = tune(svm, train.x=group$Train$X, train.y=factor(group$Train$Y), ranges=list(cost=cost_star3, gamma=gamma_levels3), tunecontrol=tune.control(cross=k_fold_cv3), kernel="radial", scale=FALSE)
# Divide and Conquer
exp_star = log2(svm_tune$best.parameters$gamma)
range = (exp_star - 3):(exp_star + 3)
gamma_levels3 = 2^range
gamma_star3 = svm_tune$best.parameters$gamma
if(index_me == 1){
gamma_star_previous = gamma_star3
}
else{
if(gamma_star_previous == gamma_star3)
break
else
gamma_star_previous = gamma_star3
}
cat(index_me, "\n")
}
return(svm_tune)
}
mjr = divide_and_conquer()
pred = predict(mjr$best.model, newdata = group$Test$X) %>% as.character() %>% as.double()
group$Test$CrystalBall = pred
}
library(tidyverse)
library(caret)
library(R.utils)
library(parallel)
library(doParallel)
# cores
cluster = makeCluster(detectCores() - 1)
registerDoParallel(cluster)
# source
sourceDirectory(paste0(getwd(), '/Functions/Feature Selection/'))
sourceDirectory(paste0(getwd(), '/Functions/Models/'))
sourceDirectory(paste0(getwd(), '/Functions/Pipeline/'))
sourceDirectory(paste0(getwd(), '/Functions/Split into Groups/'))
sourceDirectory(paste0(getwd(), '/Functions/Others/'))
# sourceDirectory('~/Documents/Languages/R/Machine Learning/Models/')
# read
dat_train <- read_csv(paste0(getwd(), '/Data/cooked_training_data.csv'))
dat_test <- read_csv(paste0(getwd(), '/Data/cooked_test_data.csv'))
# split
Train = list('group1' = get_group_data(dat_train, group_labs = 1:3),
'group2' = get_group_data(dat_train, group_labs = 4:5),
'group3' = get_group_data(dat_train, group_labs = 6))
Test = split_test_data(dat_test)
Group = list('group1' = list('Train' = Train$group1, 'Test' = Test$group1),
'group2' = list('Train' = Train$group2, 'Test' = Test$group2),
'group3' = list('Train' = Train$group3, 'Test' = Test$group3))
start_time = proc.time()
start_time = proc.time()
# Group 1
Group$group1 = machine_learning(Group$group1, no_feature_selection, svm_star)
library(tidyverse)
library(caret)
library(R.utils)
library(parallel)
library(doParallel)
# read
dat_train <- read_csv(paste0(getwd(), '/Data/cooked_training_data.csv'))
View(dat_train)
interval = 0
Y = dat_train$activity %>% unlist() %>% as.vector()
for (element in 1:(length(Y)-1)) {
if(Y[element] != Y[element+1])
interval[element] = 0
else
interval[element] = 1
}
interval_sum = rep(0, 280)
i = 1
for (element in 1:length(interval)) {
if(interval[element] == 1)
interval_sum[i] = interval_sum[i] + 1
else {
i = i + 1
interval_sum[i] = interval_sum[i] + 1
}
}
interval_sum
interval_sum = rep(0, 235)
i = 1
for (element in 1:length(interval)) {
if(interval[element] == 1)
interval_sum[i] = interval_sum[i] + 1
else {
i = i + 1
interval_sum[i] = interval_sum[i] + 1
}
}
interval_sum
